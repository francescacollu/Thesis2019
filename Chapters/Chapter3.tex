\chapter{Numerical Methods for Open Quantum Systems}
\label{chapter3}
\epigraph{Hilbert space is a big place.}{\textit{Carlton Caves}}
As we have seen in the previous chapter, the study of an open quantum system requires the knowledge of its density matrix, solution of the Lindblad master equation~\cite{presk:quant_info}:
\begin{equation*}
    \dot{\rho} = \mathcal{L}[\rho],
\end{equation*}
where
\begin{equation}
\label{eqn:lindblad_eqn}
    \mathcal{L}[\rho] \equiv -i[H, \rho] - \sum_{a=1}^{N^2-1}\gamma_a\Bigl(\frac{1}{2}L_a^{\dagger}L_a\rho + \frac{1}{2}\rho L_a^{\dagger}L_a - L_a\rho L_a^{\dagger}\Bigl),
\end{equation}
$N$ being the dimension of Hilbert space.

In such a system the number of variables scales as the square of the dimension of the Hilbert space; for example, in a spin-$\frac{1}{2}$ system made up by $n$ elements, the size of the Hilbert space $\mathcal{H}$ is $2^n$ while the \emph{liouvillian dimension} equals to $2^{2n} \times 2^{2n}$. Clearly, a direct integration of the equation~\ref{eqn:lindblad_eqn} can be made only for systems with a very limited number of elements. One only needs to note the fact that for a brute-force integration of the equation~\ref{eqn:lindblad_eqn} for a system consisting of $8$ sites, about $34$ GB of memory would be necessary\footnote{This calculation considers the fact that the liouvillian contains, in general, complex numbers, each of them valued with a single precision (32 bit).}.

Nevertheless, some techniques for exact diagonalization of Liouvillian superoperator~\ref{eqn:lindblad_eqn} has been developed. To name but one, that devised by Prosen in ref.~\cite{Prosen_2008} for studying a quadratic system of interacting fermions coupled to a set of Markovian baths, prescribes to express the~\ref{eqn:lindblad_eqn} in terms of anticommuting super-operators which act on the Fock space of density operators. 
%The numerical methods employed in open quantum system problems can be classified in those that have a wave-function approach (quantum trajectories method, mean field method) and those that decompose the system into blocks (corner-space renormalization method, matrix product density operators method).

However, as seen previously, a direct integration of master equation for an open many-body system becomes unfeasible even for relatively small sizes, so conceiving numerical strategies seems to be necessary.

One of them is based on the \emph{mean field} approach; the essential idea is considering the mean value of the Hamiltonian and, as suggested by Gutzwiller, factorizing the density matrix, defining a mean field density matrix $\rho_{MF} = \prod_j\rho_j$, where $\rho_j$ is the single site density matrix. In this way, all correlations and interactions between sites are neglected, so we can say it is a crude approximation for an interacting many-body system. In order to (partially) overcome this problem, the \emph{cluster mean field} method was developed: in this method a set of contiguous sites are isolated from the rest of the system. As explained in~\cite{PhysRevX.6.031011}, this method suggests to write the Hamiltonian as $H_{CMF} = H_\mathcal{C} + H_{\mathcal{B(C)}}$, where the first term describes the exact interactions inside the cluster while the second term represents the mean-field interactions between the cluster and the sites on its boundary $\mathcal{B(C)}$. The global density matrix will be $\rho_{CMF} = \bigotimes \rho_\mathcal{C}$, where the $\rho_\mathcal{C}$ is the density matrix of the $\mathcal{C}$th cluster. One should note that, when the cluster is made up by a single site, the method becomes the above-mentioned standard one. 

Others important numerical methods are the ones essentially based on the \emph{linked cluster expansions}~\cite{oitmaa}, in which the quantities of interest are expressed as sums of contributions from a sequence of clusters of sites; in particular, only connected clusters contribute. In~\cite{PhysRevX.6.021037} an application of this idea to the Liouvillian is done.

In this section, we will examine some of the numerical techniques used for studying open quantum systems with very different approaches.

Two of the three numerical methods analyzed in this chapter are essentially based on the Wilson's \emph{renormalization group} approach~\cite{RevModPhys.47.773} combined with the fundamental contribution of the White's \emph{density matrix renormalization group} (DMRG) method~\cite{s_white:dmrg} and are the \emph{corner-space renormalization} and the \emph{matrix product density operators} methods. They both use the idea of a system decomposition into blocks; even if the basic idea is the same, the developed approaches are actually different.

The first numerical method analyzed in the section~\ref{chapt2_qtm} is the \emph{quantum trajectories} method, based on the evolution of a Monte Carlo wave function.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Quantum Trajectories (QT) Method}
\label{chapt2_qtm}
The \emph{quantum trajectories method} is an estabilished numerical method, in which pure states are the subjects of the study, instead of density matrices. This means that, if the Hilbert space has dimension $N$, the number of involved parameters ($\sim N$) is much smaller than the one required in calculations with density matrices ($\sim N^2$). This method was originally proposed in 1992 by Dalibard, Castin and Mølmer in~\cite{PhysRevLett.68.580}, then generalized in~\cite{Molmer:93}, as a stochastic unraveling of the master equation, namely a stochastic evolution for the wave function of a system coupled to a reservoir. It has been proved~\cite{PhysRevLett.68.580, Molmer:93} the equivalence of this \emph{Monte Carlo wave-function} approach with the master equation treatment.

The idea of this method can be summarised in the following way.

First of all, given the~\ref{eqn:lindblad_eqn}, let us note that the first three terms of this equation can be regarded as the evolution performed by an effective non-Hermitian Hamiltonian, that is~\cite{PhysRevA.69.062317}:
\begin{equation*}
    H_{\text{eff}} = H_s + \text{i}K,
\end{equation*}
with
\begin{equation*}
    K = -\frac{1}{2}\sum_\mu L_{\mu}^{\dagger}L_{\mu}.
\end{equation*}
Indeed, we see that:
\begin{equation*}
    -\text{i}[H_\text{eff}, \rho] = -\text{i}[H_s, \rho] - \frac{1}{2}\sum_\mu \{L_{\mu}^{\dagger}L_{\mu}, \rho\}.
\end{equation*}

The \textbf{last term} of the~\ref{eqn:lindblad_eqn}, is the one responsible for the so-called \emph{quantum jumps}; for this reason, the representation under which we have written the~\ref{eqn:lindblad_eqn} is called \emph{quantum jump picture}~\cite{PhysRevA.69.062317}. 

At an initial time $t_0$, the density matrix of the system is in a pure state
\[
\rho(t_0) = \ket{\phi(t_0)}\bra{\phi(t_0)};
\]
after a time $dt$, it evolves to the following statistical mixture:
\begin{equation}
    \rho(t_0 + dt) = \Bigl(1-\sum_\mu dp_\mu \Bigl)\ket{\phi_0}\bra{\phi_0} + \sum_\mu dp_\mu \ket{\phi_\mu}\bra{\phi_\mu},
\end{equation}
where
\begin{equation}
    dp_\mu = \bra{\phi(t_0)}L_{\mu}^{\dagger}L_{\mu}\ket{\phi(t_0)}dt
\end{equation}
is the probability that a jump occurs; in this case, the system evolves in the state
\begin{equation}
\label{eqn:phi_mu}
    \ket{\phi_\mu} = \frac{L_\mu}{\abs{L_\mu\ket{\phi(t_0)}}}\ket{\phi(t_0)}.
\end{equation}
Otherwise, if no jump happens, the system evolves according to the effective Hamiltonian $H_{eff}$ in the following way:
\begin{equation}
    \ket{\phi_0} = \frac{(1-\textrm{i}H_{eff}dt)}{\sqrt{1-\sum_\mu dp_\mu}}\ket{\phi(t_0)}.
\end{equation}

In order to decide if the jump occurs or not, a Monte Carlo method will be integrated in this picture. Namely, an uniform distribution in the unit interval $[0,1]$ is taken under consideration; for every experiment, a pseudo-random number $\epsilon$ is chosen from this uniform distribution, i.e. a coin is tossed: depending on the result of the throw, the possible situations are the following:
\begin{itemize}
    \item if $\epsilon < \sum_\mu dp_\mu$, the system jumps to one of the states $\ket{\phi_\mu}$, defined in~\ref{eqn:phi_mu}. In particular:
    \begin{itemize}
        \item if $0 \leq \epsilon \leq dp_1$, the system jumps to $\ket{\phi_1}$;
        \item if $dp_1 < \epsilon \leq dp_2$, the system jumps to $\ket{\phi_2}$;
        \item and so on;
    \end{itemize}
    \item if $\epsilon > \sum_\mu dp_\mu$, the system evolves to the state $\ket{\phi_0}$.
\end{itemize}

This process has to be repeated as many times as $n = \frac{T}{dt}$, where $T$ is the whole elapsed time during the evolution. Let us note that $dt$ must be taken much smaller than the scales relevant for the evolution of the system.

Every \emph{experiment}, i.e. every throw of the coin, gives a different \emph{quantum trajectory}, which can be used to calculate the mean value of an observable $A$ at a certain time $t$, in this way:
\begin{equation}
    \langle A(t)\rangle = \bra{\phi_i(t)}A\ket{\phi_i(t)}.
\end{equation}
Since this results from a Monte Carlo process, we can consider the mean value over $N$ \emph{experiments}:
\begin{equation}
    \langle A(t)\rangle = \lim_{N\to\infty} \frac{1}{N}\sum_{i=1}^{N}\bra{\phi_i(t)}A\ket{\phi_i(t)}.
\end{equation}
So, to sum up we can say a few things about QT algorithm; it allows to do the so-called \emph{stochastic unraveling} of the master equation, using pure states instead of density matrices: as we have seen, this simplifies the computational complexity of the problem. However, this method has an important limit: it does not prevent the exponential growth respect to the size of the system. Trying to work this out will be a task of the methods that will be analyzed in the next two sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Corner-Space Renormalization (CSR) Method}
\label{chapter3_csr}
The numerical methods based on renormalization group à la Wilson present a computational problem due to the increase of the dimensions of the Hilbert space, while the blocks are merged; the fundamental aim of the corner-space renormalization (CSR) method~\cite{PhysRevLett.115.080604} is to deal with this problem and try to overcome the limitation mentioned above. 


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figures/wilson.png}
    \caption{Blocking scheme of Wilson's NRG method.}
    \label{fig:wilson}
\end{figure}

First of all, it is important to mention Wilson's numerical renormalization group (NRG) procedure, in which the CSR method is rooted. 

Let us consider a 1D-system made up by $N$ spins. The NRG method suggests to take two spins and consider a subspace of the corresponding Hilbert space $\mathcal{H}_2 \subset \mathcal{H}_1 \otimes \mathcal{H}_1$ of dimension $d_2 \leq d_1^2$; then, one adds a spin and consider the subset of the Hilbert space of three spins $\mathcal{H}_3 \subset \mathcal{H}_2 \otimes \mathcal{H}_1$ of dimension $d_3 \leq d_2d_1$, and so on until one arrives to consider the subset of Hilbert space $\mathcal{H}_N \subset \mathcal{H}_{N-1} \otimes \mathcal{H}_1$ with dimension $d_N \leq d_{N-1}d_1$. So, one can approximate the Hamiltonian with $P_NHP_N$, where the $P_N$ is the projector onto $\mathcal{H}_N$. If one chooses $d_N$ sufficiently small, one can diagonalize the Hamiltonian and find eigenvalues and eigenvectors. A good thing to do is fixing for all the dimensions $d_n$ the so-called \emph{bond dimension} such that 
\begin{equation*}
    d_n \leq D \quad \forall \quad n. 
\end{equation*}

We consider a one-dimensional spin chain;  the NRG method requires to break the chain into a set of identical blocks A (see fig.~\ref{fig:wilson}). So, one diagonalizes the Hamiltonian matrix $H_{AA}$ for two neighboring blocks $A$ and uses its lowest $m$ eigenstates (ordered by energy) to form a pseudo-unitary matrix $O$, employed to change the basis and make up the Hamiltonian $H_{A'}$ representing the block $A'$ twice as large (see fig.~\ref{fig:wilson}), in this way:
\begin{equation*}
    H_{A'} = OH_{AA}O^\dagger,
\end{equation*}
where $O$ is a $m\times n$ matrix (the rows of $O$ are the $m$ lowest eigenstates of $H_{AA}$) and $H_{AA}$ has dimension $n \times n$. This procedure has to be repeated using the new larger blocks $A'$ and the new effective Hamiltonian $H_{A'}$. 

\begin{figure}[H]
\centering
%\begin{subfigure}{6cm}
    \includegraphics[width=0.5\linewidth]{Figures/blocks_wilson}
    \caption{Merging of the blocks in \\NRG procedure.}
    \label{fig:blocks_wilson}
%\end{subfigure}\quad
%\begin{subfigure}{6cm}
%    \includegraphics[width=\linewidth]{Figures/blocks_dmrg}
%    \caption{Merging of blocks (block+single site) in DMRG procedure.}
%    \label{fig:blocks_dmrg}
%\end{subfigure}
%\caption{Comparisons of blocking schemes of NRG and DMRG methods.}
%\label{fig:blocks}
\end{figure}

It is worth noting that the NRG method overlooks the connections between the blocks (see fig.~\ref{fig:blocks_wilson}), so it is a good method only in the limit that the links between the blocks vanish. This weak point has been studied and overcome by White in~\cite{s_white:dmrg} where he developed the DMRG method, that will be outlined in the following section.

The CSR method can be seen as a generalization of the NRG method, a development to the case of open systems. The name refers to the idea of selecting a \emph{corner} of the Hilbert space for a lattice system, using eigenvectors of the steady-state density matrix of smaller lattices.

Let us start considering two blocks of a quantum system consisting in a certain number $N$ of sites. The CSR approach is a recursive process that begins with the calculation of the steady-state density matrix of a small block (the first \emph{block} will be a single site, in the second iteration it will be a small lattice constituted of the two blocks emerged in the previous iteration, and so on); this calculation can be done studying the spectrum of the eigenvalue of the Liouvillian and taking the zero value, considered that
\begin{equation}
    \frac{d\rho}{dt} = \mathcal{L} \rho
\end{equation}
can be seen as an eigenvalue equation.

We now consider two spatially adjacent sites: the site A and the site B. We now call $\rho^{(A)}$ the steady-state density matrix of the first site and $\rho^{(B)}$ of the latter. In order to spatially merge these two sites, let us write $\rho^{(A)}$ and $\rho^{(B)}$ in terms of an orthonormal basis; if there is no degeneracy among eigevalues of $\rho^{(A)}$ and $\rho^{(B)}$, we can use the orthonormal basis formed by their eigenvectors. Instead, if there is degeneration, a Gram-Schmidt orthonormalization process can be used to obtain an orthonormal basis starting from the ensemble of eigenvectors.

In this way, we will have:
\begin{equation}
    \begin{split}
        \rho^{(A)} & = \sum_r p_r^{(A)}\ket{\phi_r^{(A)}}\bra{\phi_r^{(A)}}, \\
        \rho^{(B)} & = \sum_{r'} p_{r'}^{(B)}\ket{\phi_{r'}^{(B)}}\bra{\phi_{r'}^{(B)}},
    \end{split}
\end{equation}
where
\begin{equation*}
    \begin{split}
        p_r^{(A)} \geq 0 \quad \forall r \quad &\textrm{and} \quad p_{r'}^{(B)} \geq 0 \quad\forall r', \\
        \sum_{r} p_r^{(A)} = 1 \quad &\textrm{ and } \quad \sum_{r'} p_{r'}^{(B)} = 1.
    \end{split}
\end{equation*}
The steady-state density matrix of the \emph{merged block} constituted of the block A and the block B will be the following:
\begin{align}
\label{rhoAUB}
    \rho^{(AUB)} &= \rho^{(A)} \otimes \rho^{(B)} = \\
    \label{rhoAUB_explicit}
    &= \sum_{r} \sum_{r'} p_r^{(A)}p_{r'}^{(B)}\ket{\phi_r^{(A)}}\ket{\phi_{r'}^{(B)}} \otimes \bra{\phi_{r'}^{(B)}}\bra{\phi_{r}^{(A)}}
\end{align}
At this point, the CSR method suggests to delineate the \emph{corner-space}; for this purpose, we consider the $M$ most probable product states coming from~\ref{rhoAUB} of the form $\ket{\phi_{r}^{(A)}}\ket{\phi_{r'}^{(B)}}$, i.e. we rank the~\ref{rhoAUB_explicit} according to the joint probability $p_r^{(A)}p_{r'}^{(B)}$. The $M$ states form an orthonormal basis that generates a subspace
\begin{equation}
\label{basis_corner}
    \Bigl\{\ket{\phi_{r_1}^{(A)}}\ket{\phi_{r_1'}^{(B)}}, \ket{\phi_{r_2}^{(A)}}\ket{\phi_{r_2'}^{(B)}}, \dots, \ket{\phi_{r_M}^{(A)}}\ket{\phi_{r_M'}^{(B)}} \Bigl\},
\end{equation}
called \emph{corner-space}, with
\begin{equation*}
    p_r_1^{(A)}p_{r_1'}^{(B)} \geq p_r_2^{(A)}p_{r_2'}^{(B)} \geq \dots \geq p_r_M^{(A)}p_{r_M'}^{(B)}.
\end{equation*}
So, now it becomes necessary doing a change of basis to the new block $A \cup B$, that is:
\begin{equation}
    H' = \widetilde{O} H_{A\cup B} \widetilde{O}^\dagger,
\end{equation}
where $\widetilde{O}$ is a pseudo-unitary\footnote{The matrix $\widetilde{O}$ is called pseudo-unitary because it is not a square matrix.} $M\times N$ matrix, where $N$ is the dimension of $H_{A\cup B}$; $\widetilde{O}$ is formed by the $M$ eigenvectors of $\rho_{A\cup B}$ living in the corner-space. This density matrix can now be used to calculate the expectation values of any observable.

The more $M$ is large, the more the results are exact, because the basis~\ref{basis_corner} spans a larger part of the Hilbert space. Therefore, at this point the value of $M$ should be increased until the convergence is reached.

In the work of~\cite{PhysRevLett.115.080604} it has been proved that convergence can be achieved with a number of states $M$ much smaller than the dimension of the Hilbert space. In the present thesis, it will be shown that this method can not be used for the analysis of systems described by a model such as that under consideration in the present work.

%The resulting dimension of the space in which, from now on - re-iterating the procedure - the system will be studied, is made smaller.

\begin{figure}
    \centering
    \includegraphics[scale=0.55]{Figures/csr_sketch}
    \caption{Sketch of the corner space renormalization method.}
    \label{fig:csr_sketch}
\end{figure}

\bigskip
In short, the algorithm is structured in the following stages, as sketched in fig.~\ref{fig:csr_sketch}:
\begin{description}
    \item[calculation] of the steady-state density matrix of a single block of the system;
    \item[merger] of two predetermined blocks;
    \item[expression] of the density matrix of the merged block in an orthonormal basis;
    \item[selection] of the M most probable states as a basis for the so-called \emph{corner-space};
    \item[increase] of the dimension M of the corner-space until the convergence of the observable is achieved.
\end{description}

A detailed description of the implementation of the algorithm can be seen in Appendix~\ref{AppendixA}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Matrix Product Density Operators (MPO) Method}
Finally, we analyze the \emph{matrix product density operators} (MPO) method.
This method is rooted in one of the first and most efficient numerical methods: the \emph{density matrix renormalization group} (DMRG)~\cite{s_white:dmrg, PhysRevB.48.10345, RevModPhys.77.259}.

Essentially, the DMRG is a generalization of Wilson's NRG method; the core of the method lies in the construction of the so-called \emph{superblock}, a block made up by 

the core of the DMRG method goes as follows: after the construction of a portion of the system, one enlarges it and in order to keep the size of the Hilbert space manageable, truncates its basis, keeping only the most relevant states, according to
\begin{equation*}
    \rho = \Tr_{E}()
\end{equation*}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{Figures/DMRG_superblockSketch.png}
    \caption{Sketch of blocks configuration in DMRG method. The rectangles represent block of $l$ and $l'$ sites respectively, while the circles represent single sites.}
    \label{fig:DMRG_superblockSketch}
\end{figure}

The MPO method is the generalization of the \emph{matrix product states} (MPS) method \cite{SCHOLLWOCK201196}, which is useful in the description of (closed) quantum many-body systems. The essential idea~\cite{Cirac_2009} behind the MPS method lies in Wilson's renormalization group; in particular, 
So, let us consider the Hilbert space $\mathcal{H}_2$; we write an arbitrary orthonormal basis $\{\ket{\beta\}}_{\beta=1}^{d_2}$ as
\begin{equation*}
    \ket{\beta}_2 = \sum_{n_1, n_2}^d_1 B_\beta^{n_1, n_2} \ket{n_1}_1 \otimes \ket{n_2}_1,
\end{equation*}
where $B_\beta^{n_1, n_2}$ are the coefficients of the basis vectors in terms of the original basis vectors $\ket{n} \in \mathcal{H}_1$ and can be expressed as
\begin{equation*}
    B_\beta^{n_1, n_2} = \sum_{\alpha=1}^d_1 A[1]_\alpha^{n_1}A[2]_{\alpha,\beta}^{n_2},
\end{equation*}
where $A[1]_\alpha^{n_1} = \delta_{n1, \alpha}$ and $A[2]_{\alpha,\beta}^{n_2} = B_\beta^{\alpha, n_2}_\beta$. So, we can now write any orthonormal basis in $\mathcal{H}_3$ in terms of $\ket{\beta}_2\otimes \ket{\ket{n_3}}$ and go on iteratively. After M steps, we have:
\begin{equation*}
    \ket{\beta}_M = \sum_{\alpha=1}^{d_{M-1}}\sum_{n_M=1}^{d_1} A[M]_{\alpha,\beta}^{n_M}\ket{\alpha}_{M-1}\otimes\ket{n_M}_1,
\end{equation*}
that is, substituting recursively the definition of $\ket{\alpha}_m$:
\begin{equation*}
    \ket{\beta}_N = \sum_{n_1,\dots,n_N=1}^{d_1} (A_1^{n_1}A_2^{n_2}\dots A_N^{n_N})_\beta \ket{n_1,\dots,n_N},
\end{equation*}
i.e.
\begin{equation}
\label{mps_general}
    \ket{\psi_{MPS}} = \sum_{n_1,\dots,n_N=1}^{d_1} A_1^{n_1}A_2^{n_2}\dots A_N^{n_N} \ket{n_1,\dots,n_N}.
\end{equation}
Because of the form of , this vectors are called \emph{matrix product states} (MPS). %\textcolor{red}{FIGURA DAL CIRAC2009 PAGINA 16}

For periodic boundary conditions the form of the~\ref{mps_general} is obtained performing the trace over their product~\cite{SCHOLLWOCK201196, PhysRevLett.93.207204}:
\begin{equation}
    \ket{\psi_{MPS}} = \sum_{n_1,\dots,n_N=1}^{d} \Tr(A_1^{n_1}A_2^{n_2}\dots A_N^{n_N}) \ket{n_1,\dots,n_N}.
\end{equation}
One should note that the total number of parameters turns out to be $N \times d \times D^2$, i.e. it is linear in $N$.

The matrix product \emph{density operators} (MPO) extend the MPS from pure to mixed states, following the idea explained, for example, in~\cite{PhysRevLett.93.207204}. The physical index is now doubled, as illustrated in fig.~\ref{fig:tensor_network}%\textcolor{red}{FIGURA}:
\begin{equation}
\label{eqn:mpdo}
    \rho_{MPO} = \sum_{i_1, \dots ,i_N = 1}^{d} \sum_{j_1, \dots ,j_N = 1}^{d} \Tr\Bigl(A_1^{i_1,j_1} \dots A_N^{i_N,j_N}\Bigl) \ket{i_1, \dots ,i_N}\bra{j_1, \dots ,j_N}.
\end{equation}
The $A^{[k]i_k,j_k}$ are $D^2 \times D^2$ tensors that can be decomposed in this way:
\begin{equation*}
    A_k^{i,j} = \sum_{a = 1}^{d_k} M_k^{i,a} \otimes (M_k^{j,a})^*.
\end{equation*}

Another way of considering the expression~\ref{eqn:mpdo} is the graphical notation to represent a tensor $A^{[k] i,j}$ sketched in figure~\ref{fig:tensor_network}. Here, every tensor is represented by a square with 4 legs: there are two of them for the bonds with the neighbours, the other two for the physical indices (the "bras" and "kets"). 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.30]{Figures/tensor_network.png}
    \caption{Graphical notation of tensors in a 1D system.}
    \label{fig:tensor_network}
\end{figure}

As regards dynamics, the long-time limit of eq.~\ref{eqn:lindblad_eqn} can be reached using a MPO ansatz for the density matrix. The steady-state solution $\rho$ is found using an algorithm based on the \emph{time evolution block decimation} (TEBD)~\cite{PhysRevLett.91.147902} scheme adapted to open systems. It is useful, for instance, to efficiently compute time-dependent properties of 1D closed quantum many-body systems as done in~\cite{PhysRevLett.93.040502}. 

Here, we can briefly see how it works~\cite{jin_biella_ross} in the case of open many-body systems in the MPO formalism. 

First of all, let us consider the~\ref{eqn:mpdo}: repeated single-value decomposition of the tensor $C_{i_1,\dots, i_N; j_1,\dots, j_N}$ can lead to
\begin{equation*}
\label{mpo_ansatz}
\begin{split}
    \rho_{MPO} = \sum_{i_1, \dots ,i_N = 1}^{d}\quad \sum_{j_1, \dots ,j_N = 1}^{d}\quad \sum_{\alpha_1,\dots, \alpha_{N-1} = 1}^\chi &(\Gamma_{1, \alpha_1}^{[1]i_1, j_1}\lambda_{\alpha_1}^{[1]})(\Gamma_{\alpha_1, \alpha_2}^{[2]i_, j_2}\lambda_{\alpha_2}^{[2]}) \dots \\  \times &(\lambda_{\alpha_{N-1}}^{[N-1]}\Gamma_{N, \alpha_{N-1}}^{[N]i_N, j_N}) ||i_1, \dots ,i_N, j_1, \dots ,j_N\rangle\rangle.
    \end{split}
\end{equation*}
where the superoperator formalism is been used in order to write the superket \\$||i_1, \dots ,i_N, j_1, \dots ,j_N\rangle\rangle$. The bond-link dimension $\chi$ can be kept under a threshold by discounting the smallest single values; moreover, it is proportional to the amount of quantum correlation between the system sites that can be embedded in $\rho_{MPO}$.

%As shown in~\cite{Prosen_2009, PhysRevLett.93.207205}, $\hat{\mathcal{L}}$ %can be decomposed into terms that involve two contiguous sites, as
%\begin{equation}
%    \hat{\mathcal{L}}[\rho] = \sum_l \hat{\mathcal{L}}_{l,l+1}[\rho].
%\end{equation}
Formally, the solution of~\ref{eqn:lindblad_eqn} can be written as
\begin{equation}
\label{eqn:lindblad_dinamics}
    \rho(t) = e^{\hat{\mathcal{L}}t}\rho(0).
\end{equation}
Following the Vidal's work~\cite{PhysRevLett.93.040502}, the liouvillian superoperator $\hat{\mathcal{L}}$ can be decomposed in this way:
\begin{equation}
    \hat{\mathcal{L}} = \hat{F} + \hat{G},
\end{equation}
where
\begin{equation}
    \begin{split}
        \hat{F} &\equiv \sum_{\text{even } l} \hat{F}^{[l]} \equiv \sum_{\text{even } l} (\hat{\mathcal{L}}_2^{l, l+1}) \\
        \hat{G} &\equiv \sum_{\text{odd } l} \hat{G}^{[l]} \equiv \sum_{\text{odd } l} (\hat{\mathcal{L}}_2^{l, l+1}), \\
        l  &= 1, \dots N-1
    \end{split}
\end{equation}
having considered a Liouvillian made of arbitrary nearest-neighbouring interaction terms.

For every small time step $\tau > 0$, the Trotter-Suzuki expansion of order $p$ for $\exp{(\hat{\mathcal{L}} t)}$ reads:
\begin{equation*}
    e^{\hat{\mathcal{L}}t} = e^{(\hat{F}+ \hat{G})t} = \prod_k \exp{(\alpha_k\hat{\mathcal{L}}_1\tau)}\dots\exp{(\beta_k\hat{\mathcal{L}}_N\tau)} + \mathcal{O}(\tau^p),
\end{equation*}
with $p$ depending on the required accuracy.

%\textcolor{red}{VEDI PRESENTAZIONE GARNET CHAN - RIVEDERE \\ AGGIUNGERE FORMALISMO SUPEROPERATORIALE}

%\noindent\rule[0.5ex]{\linewidth}{1pt}
%\noindent\rule[0.5ex]{\linewidth}{1pt}

%One can express a MPDO in terms of a pure state MPS using the \emph{purification}~\cite{nielsen_chuang} technique. The essential idea is to consider an auxiliary system with a Hilbert space of dimension $d_k$ and, after choosing an orthonormal basis $\ket{i_k, a_k}$, to write the corresponding MPS state as
%\begin{equation}
%    \ket{\psi} = \sum_{i_1,\dots,i_N} \sum_{a_1, \dots, a_N} Tr\Bigl(\prod_{k=1}^N B_k^{i_k, a_k}\Bigl) \ket{i_1a_1, \dots, i_Na_N}.
%\end{equation}
%and eventually the MPDO $\rho$ is obtained tracing over the indices referred to the auxiliary system $a_k$:
%\begin{equation}
%    \rho = Tr_a(\ket{\psi}\bra{\psi});
%\end{equation}
%this density matrix can be used to compute the expectation values of observables.